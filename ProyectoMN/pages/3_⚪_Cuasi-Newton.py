import streamlit as st
st.title("M茅todo de Cuasi-Newton")

st.markdown(
    """
    ### Definici贸n
    El m茅todo de cuasi-Newton es una t茅cnica utilizada para encontrar soluciones aproximadas a problemas de optimizaci贸n no lineal. A diferencia del m茅todo de Newton, que requiere calcular y utilizar la matriz hessiana (derivadas parciales de segundo orden) en cada iteraci贸n, el m茅todo de cuasi-Newton aproxima la matriz hessiana mediante m茅todos iterativos y actualiza esta aproximaci贸n en cada iteraci贸n.
    
    La f贸rmula b谩sica del m茅todo de cuasi-Newton para actualizar la aproximaci贸n de la matriz hessiana en cada iteraci贸n se conoce como "f贸rmula de actualizaci贸n de BFGS" (Broyden-Fletcher-Goldfarb-Shanno). 
    """
)
st.latex(r'''
   H_k = H_{k-1} - \frac{H_{k-1}s_{k-1}s_{k-1}^T H_{k-1}}{s_{k-1}^T H_{k-1} s_{k-1}} + \frac{y_{k-1}y_{k-1}^T}{y_{k-1}^T s_{k-1}}
    ''')
st.markdown(
    """Donde:"""

)
st.latex(r'''
    H_k 
    ''')
st.markdown("""es la matriz hessiana aproximada en la iteraci贸n.""")
st.latex(r'''
    k, H_{k-1}
    ''')
st.markdown('''es la matriz hessiana aproximada en la iteraci贸n.''')
st.latex(r'''
     k-1, s_{k-1}
    ''')
st.markdown('''es el vector de diferencia entre las estimaciones de par谩metros en las iteraciones k. ''')

st.latex(r'''
     k-1 (s_{k-1} = x_k - x_{k-1}), y_{k-1}
    ''')
st.markdown('''es el vector de diferencia entre los gradientes de la funci贸n objetivo en las iteraciones k y k-1.''')

st.latex(r'''
     (y_{k-1} = \nabla f(x_k) - \nabla f(x_{k-1}))
    ''')
st.markdown('''y T representa la transposici贸n de un vector o matriz.''')

st.markdown(
    """
    Es importante destacar que existen otras variantes y modificaciones del m茅todo de cuasi-Newton, como el m茅todo L-BFGS, que utiliza una aproximaci贸n limitada de la matriz hessiana. Estas variantes pueden tener f贸rmulas de actualizaci贸n ligeramente diferentes, pero la f贸rmula b谩sica de actualizaci贸n de BFGS es una de las m谩s comunes y ampliamente utilizadas.
    """
)

st.markdown(
    """
    ### Aplicaciones

    El m茅todo de cuasi-Newton, espec铆ficamente la variante conocida como BFGS (Broyden-Fletcher-Goldfarb-Shanno), es ampliamente utilizado en diversas aplicaciones en campos como la optimizaci贸n num茅rica, la ciencia de datos y el aprendizaje autom谩tico. Algunas de las aplicaciones m谩s comunes del m茅todo de cuasi-Newton son las siguientes:

     Optimizaci贸n no lineal: El m茅todo de cuasi-Newton es utilizado para encontrar m铆nimos o m谩ximos de funciones no lineales en problemas de optimizaci贸n. Se aplica en 谩reas como la optimizaci贸n de funciones objetivo en algoritmos de aprendizaje autom谩tico y la estimaci贸n de par谩metros en modelos matem谩ticos complejos.

     Aprendizaje autom谩tico: En algoritmos de aprendizaje autom谩tico, el m茅todo de cuasi-Newton se utiliza para optimizar la funci贸n de costo en modelos de regresi贸n log铆stica, redes neuronales y m谩quinas de vectores de soporte, entre otros. Ayuda a encontrar los par谩metros que mejor ajustan los datos y permiten el entrenamiento eficiente de los modelos.

     Ajuste de curvas: El m茅todo de cuasi-Newton se utiliza en el ajuste de curvas para encontrar una funci贸n que se ajuste a un conjunto de datos. Esto es 煤til en 谩reas como la interpolaci贸n y la aproximaci贸n de datos experimentales o observados.

     Modelado de sistemas f铆sicos: El m茅todo de cuasi-Newton se aplica en la resoluci贸n num茅rica de sistemas de ecuaciones diferenciales no lineales que describen fen贸menos f铆sicos. Es 煤til en 谩reas como la din谩mica de fluidos, la mec谩nica estructural y la simulaci贸n de procesos f铆sicos complejos.

     Reconstrucci贸n de im谩genes: En el procesamiento de im谩genes, el m茅todo de cuasi-Newton se utiliza para resolver problemas de reconstrucci贸n de im谩genes a partir de datos incompletos o ruidosos. Ayuda a encontrar im谩genes que sean coherentes con los datos observados y que cumplan ciertas restricciones.

    Estas son solo algunas de las aplicaciones m谩s comunes del m茅todo de cuasi-Newton. Su versatilidad y eficiencia lo convierten en una herramienta poderosa para resolver una amplia gama de problemas de optimizaci贸n no lineal en diversas 谩reas de aplicaci贸n.
    """
) 
 
st.markdown('''
    ### Ejemplo
''') 
st.video("https://www.youtube.com/watch?v=8MtnDnnULPc")


st.markdown(
    """
    ### C贸digo
    """
) 
import streamlit as st

code = '''import numpy as np

def bfgs_method(f, x0, max_iterations=100, epsilon=1e-6):
    n = len(x0)
    H = np.eye(n)  # Aproximaci贸n inicial de la matriz Hessiana inversa
    x = x0.copy()
    
    for _ in range(max_iterations):
        gradient = compute_gradient(f, x)
        if np.linalg.norm(gradient) < epsilon:
            break
        
        direction = -np.dot(H, gradient)
        step_size = backtracking_line_search(f, x, direction, gradient)
        x_new = x + step_size * direction
        
        y = compute_gradient(f, x_new) - gradient
        s = x_new - x
        
        Hy = np.dot(H, y)
        sHy = np.dot(s, Hy)
        
        H = H + np.outer(s, s) / np.dot(s, y) - np.outer(Hy, Hy) / sHy
        x = x_new
    
    return x

# Funci贸n de ejemplo: f(x) = x^2 + 2y^2
def f(x):
    return x[0]**2 + 2 * x[1]**2

# C谩lculo del gradiente
def compute_gradient(f, x):
    h = 1e-6
    gradient = np.zeros_like(x)
    for i in range(len(x)):
        x_plus_h = x.copy()
        x_plus_h[i] += h
        gradient[i] = (f(x_plus_h) - f(x)) / h
    return gradient

# B煤squeda de l铆nea mediante el m茅todo de retroceso (backtracking line search)
def backtracking_line_search(f, x, direction, gradient, alpha=0.4, beta=0.9):
    step_size = 1.0
    while f(x + step_size * direction) > f(x) + alpha * step_size * np.dot(gradient, direction):
        step_size *= beta
    return step_size

# Ejemplo de uso
x0 = np.array([1.0, 1.0])  # Punto inicial
x_opt = bfgs_method(f, x0)

print("Punto 贸ptimo:", x_opt)
print("Valor 贸ptimo:", f(x_opt))

'''
st.code(code, language='python')